{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Building Permit Data\n",
    "EDA for Toronto Bulding Permits using the CKAN API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CKAN API request methods\n",
    "Pass queries using SQL and return responses as appropriate objects: either a dataframe, float/str/int, or error out with a response code as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.ckan import Ckan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV\n",
    "Use Ckan class to export data from a Ckan package to a CSV file. Run for all relevant packages for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "def make_csv_source_path(package_id: str, dirpath: str) -> str:\n",
    "    return os.path.join(dirpath, re.sub(\"\\\\W+\", \"-\", package_id.lower()) + \".csv\")\n",
    "\n",
    "def export_csv_from_ckan(package_id: str, dirpath = \"../data/source/csv/\") -> None:\n",
    "    ckan = Ckan()\n",
    "    ckan.set_package_id(package_id)\n",
    "    ckan.get_pkg_info()\n",
    "    ckan.find_resource_endpoints()\n",
    "    ckan.export_csv(make_csv_source_path(package_id=package_id, dirpath=dirpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_ids = [\n",
    "    \"building-permits-active-permits\",\n",
    "    \"building-permits-cleared-permits\",\n",
    "    \"address-points-municipal-toronto-one-address-repository\",\n",
    "    \"neighbourhoods\",\n",
    "]\n",
    "\n",
    "for pkg_id in pkg_ids:\n",
    "    export_csv_from_ckan(package_id=pkg_id)\n",
    "    time.sleep(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas exploration\n",
    "Check the data for its features (primary keys, data types, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/source/csv/building-permits-active-permits.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the minimum columns required to create a primary key (preserve all rows from source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PERMIT_NUM', 'REVISION_NUM', 'PERMIT_TYPE', 'BUILDER_NAME']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we can use to create a hash for a primary key are:\n",
    "- `PERMIT_NUM`\n",
    "- `REVISION_NUM`\n",
    "- `PERMIT_TYPE`\n",
    "- `BUILDER_NAME`\n",
    "\n",
    "There are some really efficient hashing algos in python created by [Maruice Borgmeier](https://mauricebrg.com/2022/12/even-more-efficient-hashing-of-columns-in-a-pandas-dataframe.html). We can implement the V3 version that relies on pandas solely for typing the dataframe into strings for hashing.\n",
    "\n",
    "Hashing algos:\n",
    "- **xxHash** <-- fastest, use this\n",
    "- crc32 (fast, old, too many collisions)\n",
    "- md5 (fast, use as an alternative)\n",
    "- sha-1 (only slightly less fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from Maurice Borgmeier's efficient pandas hashing blog post:\n",
    "# https://mauricebrg.com/2022/12/even-more-efficient-hashing-of-columns-in-a-pandas-dataframe.html\n",
    "\n",
    "\n",
    "import abc\n",
    "import typing\n",
    "import xxhash\n",
    "import pandas as pd\n",
    "\n",
    "class AbstractHasher(abc.ABC):\n",
    "    \"\"\"Implement an abstract base class for the Abstract Hasher\n",
    "    This class prepares code for a concrete Hasher class to carry out dataframe hashing.\n",
    "    dataframe:\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    dataframe : pd.Dataframe\n",
    "        the pandas dataframe with columns to be hashed\n",
    "    target_columns_name : str\n",
    "        the name of the column containing the hashes\n",
    "    columns_to_hash : list[str]\n",
    "        the list of columns to be used to create the hash\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    hash :\n",
    "        An abstract hash that runs the hash methods from concrete implements\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe: pd.DataFrame\n",
    "    target_column_name: str\n",
    "    columns_to_hash: typing.List[str]\n",
    "    num_records: int\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataframe: pd.DataFrame,\n",
    "            columns_to_hash: typing.List[str], \n",
    "            target_column_name: str) -> 'AbstractHasher':\n",
    "        \"\"\"\n",
    "        Initialize the AbstractHasher class:\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        dataframe : pd.Dataframe\n",
    "            the pandas dataframe with columns to be hashed\n",
    "        target_columns_name : str\n",
    "            the name of the column containing the hashes\n",
    "        columns_to_hash : list[str]\n",
    "            the list of columns to be used to create the hash\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataframe = dataframe.copy()\n",
    "        self.target_column_name = target_column_name\n",
    "        self.columns_to_hash = columns_to_hash\n",
    "        self.num_records = len(dataframe)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def hash(self) -> pd.DataFrame:\n",
    "        \"\"\"Hash the columns\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASH_FIELD_SEPARATOR = \"|\"\n",
    "HASH_FUNCTION = xxhash.xxh32\n",
    "\n",
    "\n",
    "class PyHasher(AbstractHasher):\n",
    "    \"\"\"\n",
    "    Hasher uses itertuples instead of converting values to a list\n",
    "    \"\"\"\n",
    "\n",
    "    def hash(self) -> pd.DataFrame:\n",
    "        def hash_string_iterable(string_iterable: typing.Iterable[str]) -> str:\n",
    "            input_str = HASH_FIELD_SEPARATOR.join(string_iterable)\n",
    "            return HASH_FUNCTION(input_str.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        \"\"\"\n",
    "        Apply the hash_string_iterable method to the specified columns of an input \n",
    "        dataframe that was typed into a str, then converted into an iter object where \n",
    "        each row is a tuple. This creates a series with the same number of items as \n",
    "        the input dataframe, and can be inserted into the specified hash column name.\n",
    "        \"\"\"\n",
    "        \n",
    "        hashed_series = pd.Series(\n",
    "            map(\n",
    "                hash_string_iterable,\n",
    "                self.dataframe[self.columns_to_hash]\n",
    "                .astype(str)\n",
    "                .itertuples(index=False, name=None),\n",
    "            ),\n",
    "            index=self.dataframe.index,\n",
    "        )\n",
    "\n",
    "        self.dataframe[self.target_column_name] = hashed_series\n",
    "\n",
    "        return self.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PK_COLS = ['PERMIT_NUM', 'REVISION_NUM', 'PERMIT_TYPE', 'BUILDER_NAME']\n",
    "\n",
    "df = PyHasher(dataframe=df, columns_to_hash=PK_COLS, target_column_name=\"permit_id\").hash()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore address data\n",
    "Explore joining address data to obtain neighbourhood data and geolocation points. There are two possible ways to show 'hotspots' of renovations: \n",
    "1. By neighbourhood, or geofenced by neighbourhood, and\n",
    "2. By permit value. \n",
    "\n",
    "(1) and (2) can be combined to provide a sense of moneyh being spent in high-activity neighbourhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc = pd.read_csv(\"../data/source/csv/address-points-municipal-toronto-one-address-repository.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc[\"WARD_NAME\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: There is no current link between address points and neighbourhoods. Therefore, we need to geofence the Address Point data to annotate it with the 158 neighbourhoods to which each address belongs. This can be done once a month in case there are new addresses or plots that come up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['STREET_NUM', 'STREET_NAME', 'STREET_TYPE', 'GEO_ID']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GEO_ID'].head().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc[df_loc['ADDRESS_POINT_ID'].isin(df['GEO_ID'].head().to_list())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that **`GEO_ID` from the *building permits* table = `ADDRESS_POINT_ID` from the *address points* table**. This simplifies enriching the building permits database with neighbourhood and goelocation points further in the analysis pipeline. Next steps are to continue EDA with the building permits data to create a draft of the main transformations required to get from the data to an analytics dashboard, then modularize transformations to simplify the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Join all building permit data\n",
    "We want to combine **active** and **cleared** permit data to create a full dataset of permits with their dates such that we can window all permits within a given date filter. These joins and appends might be changed into a minimal version of transformations that are run on filtered queries made to the CKAN api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleared = pd.read_csv(\"../data/source/csv/building-permits-cleared-permits.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all([c in df.columns for c in df_cleared.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "        df.drop(\"_id\", axis=1),\n",
    "        PyHasher(df_cleared.drop(\"_id\", axis=1), PK_COLS, \"permit_id\").hash()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cols = [\n",
    "    'PERMIT_NUM', 'REVISION_NUM', 'PERMIT_TYPE', 'WARD_GRID', 'APPLICATION_DATE', 'ISSUED_DATE',\n",
    "       'COMPLETED_DATE', 'STATUS', 'DESCRIPTION', 'CURRENT_USE',\n",
    "       'PROPOSED_USE', \n",
    "       'EST_CONST_COST', 'BUILDER_NAME', 'permit_id'\n",
    "]\n",
    "df[show_cols].sort_values(\"APPLICATION_DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    sum(df[\"APPLICATION_DATE\"].isna()),\n",
    "    sum(df[\"ISSUED_DATE\"].isna()),\n",
    "    sum(df[\"COMPLETED_DATE\"].isna()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"APPLICATION_DATE\", \"ISSUED_DATE\", \"COMPLETED_DATE\"]].isnull().all(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: There are permit records with at least one date, therefore we need to create an \"EFFECTIVE_DATE\" column that pulls the latest date available between Application, Issued, and Completed dates. Since Completed > Issued > Application dates, the effective date will be the date that appears first in that order.\n",
    "\n",
    "Alternatively: we choose the maximum date present in the record. We will use this strategy as it is quicker to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = [\"APPLICATION_DATE\", \"ISSUED_DATE\", \"COMPLETED_DATE\"]\n",
    "df[\"effective_date\"] = df[date_columns].fillna(\"\").max(axis=1, skipna=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Windowing data\n",
    "Experiment with windowing data to capture the wards with the highest permit activity. We want to see which wards have the highest activity in permit value and the most permits within the last month, quarter, and year. We also want to see the trend for each ward (permit value and number of permits) over time. Eventually we will create a static crosswalk between neighbourhoods and address points to map each address to a neighbourhood.\n",
    "\n",
    "We will opt to use the `dt.to_period` conversion for the date series. This function depends on a frequency argument which is a representation of the [date windowing defined by pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-period-aliases). We'll use `M` for **monthly**, `Q` for **quarterly**, and `Y` for **yearly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique(df: pd.DataFrame) -> list:\n",
    "    return [{col: df_loc[col].nunique()} for col in df_loc.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_PERMIT_TYPES = [\n",
    "    'Residential Building Permit',\n",
    "    'Mechanical(MS)', 'Plumbing(PS)', 'Multiple Use Permit',\n",
    "    'Change of Use Permit', 'Building Additions/Alterations',\n",
    "    'Small Residential Projects', 'Conditional Permit',\n",
    "    'Fire/Security Upgrade', 'Designated Structures',\n",
    "    'Drain and Site Service', 'Partial Permit',\n",
    "    'Demolition Folder (DM)', 'New Houses', 'New Building',\n",
    "]\n",
    "filtered = df.loc[\n",
    "    (df[\"STATUS\"] != \"Cancelled\") &\n",
    "    (df[\"PERMIT_TYPE\"].isin(ALLOWED_PERMIT_TYPES))\n",
    "]\n",
    "filtered = filtered[[\"GEO_ID\", \"effective_date\", \"EST_CONST_COST\"]]\n",
    "filtered = filtered.merge(df_loc[[\"ADDRESS_POINT_ID\", \"WARD_NAME\"]], left_on=\"GEO_ID\", right_on=\"ADDRESS_POINT_ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique(items) -> int:\n",
    "    return len(items.dropna().unique())\n",
    "\n",
    "def convert_cost_column(df:pd.DataFrame, cost_col: str) -> pd.DataFrame:\n",
    "    df[cost_col] = pd.to_numeric(df[cost_col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame, cost_col: str) -> pd.DataFrame:\n",
    "    df = convert_cost_column(df, cost_col)\n",
    "    return df\n",
    "\n",
    "def group_by_time(df:pd.DataFrame, date_col: str, group_cols: list, grouping: str = \"month\") -> pd.api.typing.DataFrameGroupBy:\n",
    "    GROUPING = {\n",
    "        \"month\": \"M\",\n",
    "        \"quarter\": \"Q\",\n",
    "        \"year\": \"Y\",\n",
    "    }\n",
    "    df[grouping] = pd.to_datetime(df[date_col]).dt.to_period(freq=GROUPING[grouping])\n",
    "    return df.groupby(group_cols + [grouping])\n",
    "\n",
    "def get_ward_period_data(\n",
    "        df: pd.DataFrame,\n",
    "        date_col: str,\n",
    "        group_cols: list,\n",
    "        cost_col: str,\n",
    "        property_col: str, \n",
    "        grouping: str,\n",
    "    ) -> pd.DataFrame:\n",
    "    grouped = group_by_time(\n",
    "        df=df, \n",
    "        date_col=date_col, \n",
    "        group_cols=group_cols, \n",
    "        grouping=grouping\n",
    "    )\n",
    "    return grouped.agg(\n",
    "            sum_est_costs = pd.NamedAgg(column=cost_col, aggfunc=\"sum\"),\n",
    "            num_permits = pd.NamedAgg(column=property_col, aggfunc=\"count\"),\n",
    "            properties_with_permit = pd.NamedAgg(column=property_col, aggfunc=count_unique),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_COLS = [\"WARD_NAME\"]\n",
    "get_ward_period_data(\n",
    "    df=preprocess_df(df, \"EST_CONST_COST\"), \n",
    "    date_col=\"effective_date\", \n",
    "    group_cols=GROUP_COLS, \n",
    "    cost_col=\"EST_CONST_COST\", \n",
    "    property_col=\"ADDRESS_POINT_ID\",\n",
    "    grouping=\"month\",\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Try to geofence with geopandas\n",
    "Can we create a neighbourhood crosswalk with geopandas and intersections? Find where `address-points-municipal-toronto.csv` address points lie within the neighbourhood shapes in `neighbourhoods.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely\n",
    "import json\n",
    "\n",
    "def geojson_parse(geojson_string: str) -> object:\n",
    "    return shapely.geometry.shape(json.loads(geojson_string))\n",
    "\n",
    "HOOD_KEEP_COLS = ['AREA_ID', 'AREA_SHORT_CODE', 'AREA_NAME', 'geometry']\n",
    "df_hood = pd.read_csv(\"../data/source/csv/neighbourhoods.csv\")[HOOD_KEEP_COLS]\n",
    "df_hood['geometry'] = df_hood['geometry'].apply(geojson_parse)\n",
    "df_hood = gpd.GeoDataFrame(df_hood)\n",
    "\n",
    "ADDRESS_KEEP_COLS = ['ADDRESS_POINT_ID', 'geometry']\n",
    "df_loc_geo = df_loc[ADDRESS_KEEP_COLS].copy()\n",
    "df_loc_geo['geometry'] = df_loc_geo['geometry'].apply(geojson_parse)\n",
    "df_loc_geo = gpd.GeoDataFrame(df_loc_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intersect = df_loc_geo.overlay(df_hood, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intersect.drop(columns='geometry').to_csv(\"../data/analysis/csv/address-neighbourhoods.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intersect.drop(columns='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intersect = pd.read_csv(\"../data/analysis/csv/address-neighbourhoods.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Stats By Neighbourhood (tying it all together)\n",
    "Now that we've annotated all properties by their neighbourhood, we can run our previous analyses on a per neighbourhood basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_permits(\n",
    "        df: pd.DataFrame,\n",
    "        allowed_permit_types: list,\n",
    "        exclude_status: str = \"Cancelled\",\n",
    "        permit_type_col: str = \"PERMIT_TYPE\",\n",
    "        status_col: str = \"STATUS\",\n",
    "    ) -> pd.DataFrame:\n",
    "    return df.loc[\n",
    "        (df[status_col] != exclude_status) &\n",
    "        (df[permit_type_col].isin(allowed_permit_types))\n",
    "    ]\n",
    "\n",
    "def prepare_and_merge_df(\n",
    "        df: pd.DataFrame,\n",
    "        df_right: pd.DataFrame,\n",
    "        allowed_permit_types: list,\n",
    "        merge_params: dict,\n",
    "        cost_col:str = \"EST_CONST_COST\",\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    df = filter_permits(df, allowed_permit_types=allowed_permit_types)\n",
    "    df = convert_cost_column(df.copy(), cost_col=cost_col)\n",
    "    \n",
    "    return df[merge_params[\"left_cols\"]].merge(\n",
    "        df_right[merge_params[\"right_cols\"]],\n",
    "        left_on=merge_params[\"left_on\"], \n",
    "        right_on=merge_params[\"right_on\"], \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "ALLOWED_PERMIT_TYPES = [\n",
    "    'Residential Building Permit', 'Small Residential Projects', \n",
    "    # 'Conditional Permit', 'Drain and Site Service', \n",
    "    # 'Mechanical(MS)', 'Plumbing(PS)', 'Multiple Use Permit',\n",
    "    # 'Change of Use Permit', 'Building Additions/Alterations',\n",
    "    # 'Partial Permit', 'Demolition Folder (DM)', \n",
    "    'New Houses', \n",
    "    #'New Building',\n",
    "]\n",
    "MERGE_PARAMS = {\n",
    "    \"left_cols\": [\"GEO_ID\", \"effective_date\", \"EST_CONST_COST\"],\n",
    "    \"right_cols\": [\"ADDRESS_POINT_ID\", \"AREA_NAME\"],\n",
    "    \"left_on\": \"GEO_ID\",\n",
    "    \"right_on\": \"ADDRESS_POINT_ID\",\n",
    "}\n",
    "df_prepped = prepare_and_merge_df(\n",
    "    df, df_right=df_intersect, \n",
    "    allowed_permit_types=ALLOWED_PERMIT_TYPES,\n",
    "    merge_params=MERGE_PARAMS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEO_ID</th>\n",
       "      <th>effective_date</th>\n",
       "      <th>EST_CONST_COST</th>\n",
       "      <th>ADDRESS_POINT_ID</th>\n",
       "      <th>AREA_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10575758</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>10575758</td>\n",
       "      <td>South Riverdale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12544613</td>\n",
       "      <td>2000-07-12</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>12544613</td>\n",
       "      <td>Roncesvalles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8190271</td>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>8190271</td>\n",
       "      <td>South Parkdale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>803583</td>\n",
       "      <td>2001-04-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>803583</td>\n",
       "      <td>High Park North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>803583</td>\n",
       "      <td>2000-04-26</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>803583</td>\n",
       "      <td>High Park North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141348</th>\n",
       "      <td>493031</td>\n",
       "      <td>2018-09-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>493031</td>\n",
       "      <td>Bedford Park-Nortown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141349</th>\n",
       "      <td>521259</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>521259</td>\n",
       "      <td>Bedford Park-Nortown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141350</th>\n",
       "      <td>546964</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>546964</td>\n",
       "      <td>Bedford Park-Nortown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141351</th>\n",
       "      <td>513957</td>\n",
       "      <td>2018-09-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>513957</td>\n",
       "      <td>Bedford Park-Nortown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141352</th>\n",
       "      <td>504000</td>\n",
       "      <td>2018-09-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>504000</td>\n",
       "      <td>Bedford Park-Nortown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141353 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          GEO_ID effective_date  EST_CONST_COST ADDRESS_POINT_ID  \\\n",
       "0       10575758     2000-01-04         20000.0         10575758   \n",
       "1       12544613     2000-07-12         50000.0         12544613   \n",
       "2        8190271     2000-02-16         50000.0          8190271   \n",
       "3         803583     2001-04-10             0.0           803583   \n",
       "4         803583     2000-04-26         10000.0           803583   \n",
       "...          ...            ...             ...              ...   \n",
       "141348    493031     2018-09-14             NaN           493031   \n",
       "141349    521259     2018-09-17             NaN           521259   \n",
       "141350    546964     2018-09-17             NaN           546964   \n",
       "141351    513957     2018-09-14             NaN           513957   \n",
       "141352    504000     2018-09-14             NaN           504000   \n",
       "\n",
       "                   AREA_NAME  \n",
       "0            South Riverdale  \n",
       "1               Roncesvalles  \n",
       "2             South Parkdale  \n",
       "3            High Park North  \n",
       "4            High Park North  \n",
       "...                      ...  \n",
       "141348  Bedford Park-Nortown  \n",
       "141349  Bedford Park-Nortown  \n",
       "141350  Bedford Park-Nortown  \n",
       "141351  Bedford Park-Nortown  \n",
       "141352  Bedford Park-Nortown  \n",
       "\n",
       "[141353 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_COLS = [\"AREA_NAME\"]\n",
    "df_processed = get_ward_period_data(\n",
    "    df=df_prepped, \n",
    "    date_col=\"effective_date\", \n",
    "    group_cols=GROUP_COLS, \n",
    "    cost_col=\"EST_CONST_COST\",\n",
    "    property_col=\"ADDRESS_POINT_ID\", \n",
    "    grouping=\"month\"\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop_count = df_intersect.groupby(\"AREA_NAME\").agg(current_property_count=pd.NamedAgg(column=\"ADDRESS_POINT_ID\", aggfunc=count_unique)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_processed.merge(df_prop_count, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WE DID IT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_csv(\"../data/analysis/csv/neighbourhood-permit-summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics: Explore the new data.\n",
    "An example of plotting the latest data by **month** and by **number of permits**: we see that the `Englemount-Lawrence` neighbourhood is seeing high amounts of residential permit activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.loc[(df_processed[\"month\"] < \"2024-05\")].sort_values([\"month\", \"num_permits\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we filter for records only for the `Englemount-Lawrence` neighbourhood and sort by **month**: we see that this area had a huge permit influx in April of 2024, and hasn't seen much residential permit activity before or after (thus far)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.loc[df_processed['AREA_NAME'] == \"Englemount-Lawrence\"].sort_values(\"month\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we filter for records only for the `Leaside-Bennington` neighbourhood and sort by **month**: we see that this area hasn't eased up in the last few months -- lots of renos going on, well above 3M dollars per month. This area is known to be a high-income neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.loc[df_processed['AREA_NAME'] == \"Leaside-Bennington\"].sort_values(\"month\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.loc[(df_processed[\"month\"] > \"2022-12\") & (df_processed[\"month\"] < \"2024-05\") & (df_processed[\"num_permits\"] > 0)].sort_values([\"month\", \"num_permits\"], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.loc[df_processed['AREA_NAME'] == \"Black Creek\"].sort_values(\"month\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps: Chlorpleth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Clorpleth (heatmap) of permit numbers and permit costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
